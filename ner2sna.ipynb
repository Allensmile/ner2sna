{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text to Networks\n",
    "Or, how you can extract meaningful information from raw text and use it to analyze social networks.\n",
    "\n",
    "You're probably drowning in text. Whether it is email or the news, we are pummeled with it. In this guide I'm going to walk you through a strategy for making sense of massive troves of unstructured text. These strategies are actively employed for legal e-discovery and within law enforcement and the intelligence community. Have fun!\n",
    "\n",
    "#### Overview\n",
    "\n",
    "In this guide we'll take a set of documents, extract entities from within them, and develop a social network based on entity document co-occurrence. This can be a useful approach for getting a sense of which entities exist in a set of documents and how those entities might be related. I'll talk more about using document co-occurrence as the mechanism for drawing an edge in a social network graph later.\n",
    "\n",
    "This guide relies on two primary pieces of software for natural language processing. I'll be using the [Natural Language Toolkit](http://www.nltk.org/) for text pre-processing and [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/) running on Docker for entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Set Up\n",
    "First, we need to get Core NLP running on Docker. If you're not familiar with Docker, that's ok! It's a really easy to use containerization service. The concept is that anywhere you can run docker you can run a docker container. Period. No need to worry about dependency management, just get docker running and pull down the container you need. Easy.\n",
    "\n",
    "### Installing Docker\n",
    "Docker has great instructions for installing Docker. I'm using a Mac so I followed their [Mac OSX Docker installation guide](https://docs.docker.com/docker-for-mac/install/). If you're using Windows check out their [Windows install guide.](https://docs.docker.com/docker-for-windows/install/). If you're using Linux I'm pretty sure you'll be able to get Docker installed on your own.\n",
    "\n",
    "To verify the installation was successful go to your command line and try running:\n",
    "\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "\n",
    "You should an empty the docker container listing looks like (I truncated a couple columns, but you get the idea):\n",
    "```\n",
    "CONTAINER ID        IMAGE               COMMAND             CREATED\n",
    "```      \n",
    "If this isn't empty you already had docker running with a container. If you are not able to run the `docker` or `docker ps` commands from your command line, **STOP**. You need to get this installed before continuing.\n",
    "\n",
    "### Installing the Core NLP container\n",
    "This part is pretty easy. You just need to run the following command at your command line:\n",
    "```\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t motiz88/corenlp\n",
    "```\n",
    "This will pull motiz88's Docker port of Core NLP and run it using port 9000. This means that port 9000 from the container will be forwarded to port 9000 on your localhost (your computer). So, you can access the Core NLP API over `http://localhost:9000`. Note that this is a fairly large container so it may take a few minutes to download and install.\n",
    "\n",
    "To make sure that the server is running, in your browser go to http://localhost:9000. You should see:\n",
    "![Stanford Core NLP Server](core_nlp_server.png \"Core NLP Server\")\n",
    "\n",
    "If you don't, don't move forward until you can verify the Core NLP server is running. You might try `docker ps` to see if the container is listed. If it is, you can scope out the logs with `docker logs coreNLP`. If it *is* running feel free to play around with the server UI. Input some text to get a feel for how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction with Core NLP Server\n",
    "To use Core NLP Server, we are going to leverage the `pycorenlp` Python wrapper which can be installed with `pip install pycorenlp`. Once that's installed, you can instantiate a connection with the coreNLP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the basic functionality by feeding a few sentences of text to the coreNLP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output object has keys: dict_keys(['sentences'])\n",
      "Each sentence object has keys: dict_keys(['index', 'parse', 'tokens'])\n"
     ]
    }
   ],
   "source": [
    "text = (\"Bill and Ted are excellent! \"\n",
    "        \"Pusheen Smith and Jillian Marie walked along the beach; Pusheen led the way. \"\n",
    "        \"Pusheen wanted to surf, but fell off the surfboard. \"\n",
    "        \"They are both friends with Jean Claude van Dam, Sam's neighbor.\")\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'ner',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "\n",
    "print('The output object has keys: {}'.format(output.keys()))\n",
    "print('Each sentence object has keys: {}'.format(output['sentences'][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `output` object, as you can see for yourself is extremely verbose. It's comprised of a top-level `key` called `sentences` which contains one object per sentence. Each `sentence` object has an array of `token` objects that can be accessed at `output['sentences'][i]['tokens']` where `i` is the index (e.g. 0, 1, 2, etc) of the sentence of interest.\n",
    "\n",
    "What is a `token` you ask? Typically in natural language processing (NLP) when you process text you want to `tokenize` it. This means splitting the text into its respective components at the word and punctuation level. So, the sentence `'The quick brown fox jumped over the lazy dog.'` would be tokenized into an array that looks like:\n",
    "`['The','quick','brown','fox','jumped','over','the','lazy','dog','.']`. Some tokenizers ignore punctuation; others retain it.\n",
    "\n",
    "You can print out the `output` if you're interested in seeing what it looks like. That said, we need to be able to identify the people that the `ner` or Named Entity Recognition module discovered. So, let's go ahead and define a function which takes a set of sentence tokens and finds the tokens which were labeled as `PERSON`. This gets a little tricky as individual tokens can be labeled as `PERSON` when they actually correspond to the same person. For example, the tokens `Jean`, `Claude`, `van`, and `Dam` all correspond to the same person. So, the function below take `tokens` which are contiguous (next to one another) within the same sentence and combines them into the same person entity. Perfect!\n",
    "\n",
    "*By the way, this `proc_sentence` function is not very Pythonic. Ideas for doing this more efficiently are welcome!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_sentence(tokens):\n",
    "    \"\"\"\n",
    "    Takes as input a set of tokens from Stanford Core NLP output and returns \n",
    "    the set of peoplefound within the sentence. This relies on the fact that\n",
    "    named entities which are contiguous within a sentence should be part of \n",
    "    the same name. For example, in the following:\n",
    "    [\n",
    "        {'word': 'Brandon', 'ner': 'PERSON'},\n",
    "        {'word': 'Rose', 'ner': 'PERSON'},\n",
    "        {'word': 'eats', 'ner': 'O'},\n",
    "        {'word': 'bananas', 'ner': 'O'}\n",
    "    ]\n",
    "    we can safely assume that the contiguous PERSONs Brandon + Rose are part of the \n",
    "    same named entity, Brandon Rose.\n",
    "    \"\"\"\n",
    "    people = set()\n",
    "    token_count = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if token_count < len(tokens):\n",
    "            person = ''\n",
    "            token = tokens[token_count]\n",
    "            if token['ner'] == 'PERSON':\n",
    "                person += token['word'].lower()\n",
    "                checking = True\n",
    "                while checking == True:\n",
    "                    if token_count + 1 < len(tokens):\n",
    "                        if tokens[token_count + 1]['ner'] == 'PERSON':\n",
    "                            token_count += 1\n",
    "                            person += ' {}'.format(tokens[token_count]['word'].lower())\n",
    "                        else:\n",
    "                            checking = False\n",
    "                            token_count += 1\n",
    "                    else:\n",
    "                        checking = False\n",
    "                        token_count += 1\n",
    "            else:\n",
    "                token_count += 1\n",
    "            if person != '':\n",
    "                people.add(person)\n",
    "    return people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the people which we can extract from each of the sentences. Note that the output of the `proc_sentence` function is a `set`, which means that it will only contain unique people entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ted', 'bill'}\n",
      "{'pusheen smith', 'pusheen', 'jillian marie'}\n",
      "{'pusheen'}\n",
      "{'sam', 'jean claude van dam'}\n"
     ]
    }
   ],
   "source": [
    "for sent in output['sentences']:\n",
    "    people = proc_sentence(sent['tokens'])\n",
    "    print(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we receive a `set` of the extracted people entities from each sentence. We can join the results with a superset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pusheen', 'bill', 'sam', 'jean claude van dam', 'jillian marie', 'ted', 'pusheen smith'}\n"
     ]
    }
   ],
   "source": [
    "people_super = set()\n",
    "for sent in output['sentences']:\n",
    "    people = proc_sentence(sent['tokens'])\n",
    "    for person in people:\n",
    "        people_super.add(person)\n",
    "\n",
    "print(people_super)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good, except notice that we see to items for Pusheen: `'pusheen'` and `'pusheen smith'`. We've done a decent job of entity extraction, but we need to take some additional steps for entity resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Resolution with Fuzzywuzzy\n",
    "\n",
    "If entity extraction is the process of finding entities (in this case, people) within a body of text then entity resolution is the process of putting like with like. As humans we know that `pusheen` and `pusheen smith` are the same person. How do we get a computer to do the same?\n",
    "\n",
    "There are many approaches that you can take for this, but we are going to use fuzzy deduplication found within a Python package called [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) (`pip install fuzzywuzzy`). Specifically, we'll use the fuzzy deduplication function (shameless plug, this is something I contributed to the fuzzywuzzy project). We can use the defaults, however you are welcome to tune the [parameters](https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py#L167-L193).\n",
    "\n",
    "Note that you may be asked to optionally install `python-Levenshtein` to speed up `fuzzywuzzy`; you can do this with `pip install python-Levenshtein`.\n",
    "\n",
    "As an example of what fuzzy deduping is, let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy.process import dedupe as fuzzy_dedupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a list containing duplicates where some entities are partial representations of the other (`pusheen` vs. `pusheen smith`). Using fuzzywuzzy's dedupe function we can take care of this pretty easily. Fuzzywuzzy defaults to returning the longest representation of the resolved entity as it assumes this contains the most information. So, we expect to see `pusheen` resolve to `pusheen smith`. Also, fuzzywuzzy can handle slight mispellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pusheen smith', 'bill', 'sam', 'jean claude van dam', 'jillian marie', 'ted'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_dupes = list(people_super)\n",
    "fuzzy_dedupe(contains_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a useful list of entities to me! Now, I'll incorporate fuzzy deduplication into a function called `get_entities`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some data\n",
    "For this guide I'll be using a selection of news articles from Breitbart's Big Government section. Who knows, maybe we'll gain some insights into the networks at play in \"Big Government.\" Could be fun.\n",
    "\n",
    "To get the articles, I'm using [Newspaper](https://github.com/codelucas/newspaper). I'm going to scrape about 150 articles off the [Breitbart Big Government section](http://www.breitbart.com/big-government/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to profile the site to find articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart = newspaper.build('http://www.breitbart.com/big-government/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 10 articles\n",
      "Obtained 20 articles\n",
      "Obtained 30 articles\n",
      "Obtained 40 articles\n",
      "Obtained 50 articles\n",
      "Obtained 60 articles\n",
      "Obtained 70 articles\n",
      "Obtained 80 articles\n",
      "Obtained 90 articles\n",
      "Obtained 100 articles\n",
      "Obtained 110 articles\n",
      "Obtained 120 articles\n",
      "Obtained 130 articles\n",
      "Obtained 140 articles\n",
      "Obtained 150 articles\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "count = 0\n",
    "for article in breitbart.articles:\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    corpus.append(text)\n",
    "    if count % 10 == 0 and count != 0:\n",
    "        print('Obtained {} articles'.format(count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this type of scraping can lead to your IP address getting flagged by some news sites, let's make sure to save our corpus. If you have a hard time using `newspaper` to get data you can just load up the data from `corpus.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'a') as fp:\n",
    "    count = 0\n",
    "    for item in corpus:\n",
    "        loaded = item.encode('utf-8')\n",
    "        loaded_j = {count: loaded}\n",
    "        fp.write(json.dumps(loaded_j) + '\\n')\n",
    "        count += 1\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read back in the data we wrote to disk in the format of:\n",
    "```\n",
    "data[index]: {'article': 'article text'}\n",
    "```\n",
    "where the index is the order we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "with open('corpus.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        item = json.loads(line)\n",
    "        key = int(list(item.keys())[0])\n",
    "        value = list(item.values())[0].encode('ascii','ignore')\n",
    "        data[key] = {'article':value}\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the entities for each of the articles we've grabbed. We'll write the results back to the `data` dictionary in the format:\n",
    "```\n",
    "data[index]: {\n",
    "              'article': article text,\n",
    "              'people': [person entities]\n",
    "             }\n",
    "```\n",
    "\n",
    "Let's make a function that wraps up both using the Core NLP Server and Fuzzywuzzy to return the correct entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_article(article):\n",
    "    \"\"\"\n",
    "    Wrapper for coreNLP and fuzzywuzzy entity extraction and entity resolution.\n",
    "    \"\"\"\n",
    "    output = nlp.annotate(article, properties={\n",
    "      'annotators': 'ner',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    \n",
    "    people_super = set()\n",
    "    for sent in output['sentences']:\n",
    "        people = proc_sentence(sent['tokens'])\n",
    "        for person in people:\n",
    "            people_super.add(person)\n",
    "\n",
    "    contains_dupes = list(people_super)\n",
    "    \n",
    "    deduped = fuzzy_dedupe(contains_dupes)\n",
    "    \n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process each article we downloaded. Note that sometimes `newspaper` will return an empty article so let's make sure that we don't try to send those ones to Core NLP Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_keys = []\n",
    "for key in data:\n",
    "     # makes sure that the article actually has text\n",
    "    if data[key]['article'] != '':\n",
    "        people = proc_article(str(data[key]['article']))\n",
    "        data[key]['people'] = people\n",
    "    # if it's an empty article, let's save the key in `fail_keys`\n",
    "    else: \n",
    "        fail_keys.append(key)\n",
    "\n",
    "# now let's ditch any pesky empty articles\n",
    "for key in fail_keys:\n",
    "    data.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually generate the network graph. I'll use the excellent Python library `networkx` to build the network graph. To do this, I need to generate a dictionary of entities where each key is a unique entity and the values are a list of vertices that entity is connected to via an edge. For example, here we are indicating that George Clooney is connected to Bill Murray, Brad Pitt, and Seth Myers and has the highest degree centrality in the social network (due to having the highest number of edges).\n",
    "```\n",
    "{'George Clooney': ['Bill Murray', 'Brad Pitt', 'Seth Myers'],\n",
    " 'Bill Murray': ['Brad Pitt', 'George Clooney'],\n",
    " 'Seth Myers: ['George Clooney'],\n",
    " 'Brad Pitt': ['Bill Murray', 'George Clooney']\n",
    " '}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "entities = {}\n",
    "\n",
    "for key in data:\n",
    "    people = data[key]['people']\n",
    "    \n",
    "    doc_ents = []\n",
    "    for person in people:\n",
    "        if ' ' in person and len(person) > 10:\n",
    "            doc_ents.append(person)\n",
    "    \n",
    "    for ent in doc_ents:\n",
    "        try:\n",
    "            entities[ent].extend([doc for doc in doc_ents if doc != ent])\n",
    "        except:\n",
    "            entities[ent] = [doc for doc in doc_ents if doc != ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_graph(ent_dict):\n",
    "\n",
    "    index = ent_dict.keys()\n",
    "    \n",
    "    g = nx.Graph()\n",
    "\n",
    "    for ind in index:\n",
    "        ents = ent_dict[ind]\n",
    "\n",
    "        # Add previously unseen entities as nodes\n",
    "        for ent in ents:\n",
    "            if ent not in g:\n",
    "                g.add_node(ent, dict(\n",
    "                    name = ent,\n",
    "                    type = 'person',\n",
    "                    degree = str(len(ents))))\n",
    "\n",
    "    for ind in index:\n",
    "        ent = ent_dict[ind]\n",
    "        \n",
    "        for edge in ent:\n",
    "            if edge in index:\n",
    "                new_edge = (ind,edge)\n",
    "                if new_edge not in g.edges(): \n",
    "                    g.add_edge(ind, edge)\n",
    "        \n",
    "    js = json_graph.node_link_data(g)\n",
    "    js['adj'] = g.adj\n",
    "    return (g, js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = network_graph(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these types of graphs are best for exploration as interactives, we are going to rely on some javascript and HTML (see the repo for more detail) to render the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "\n",
    "G_ = graph[0]\n",
    "d = json_graph.node_link_data(G_) # node-link format to serialize\n",
    "# write json\n",
    "json.dump(d, open('force/force.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our output as an iframe within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe height=500px width=100% src='force/force.html'></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe height=500px width=100% src='force/force.html'></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
